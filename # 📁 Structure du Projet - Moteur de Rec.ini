# ğŸ“ Structure du Projet - Moteur de Recherche ML/DL

```
search_engine/
â”‚
â”œâ”€â”€ ğŸ“„ emb.db                      # Base de donnÃ©es SQLite (162 MB)
â”‚                                  # Contient les embeddings et mÃ©tadonnÃ©es
â”‚
â”œâ”€â”€ ğŸ“„ query_cache.pkl             # Cache des requÃªtes (gÃ©nÃ©rÃ© automatiquement)
â”‚
â”œâ”€â”€ ğŸ indexer_optimized.py        # â­ CÅ’UR ML/DL
â”‚   â”‚                              # - Bi-encoder (all-MiniLM-L6-v2)
â”‚   â”‚                              # - Cross-encoder (ms-marco-MiniLM-L-6-v2)
â”‚   â”‚                              # - TF-IDF/BM25
â”‚   â”‚                              # - RRF Fusion
â”‚   â”‚                              # - Query Expansion
â”‚   â”‚                              # - Page Rank
â”‚   â””â”€â”€ Classes:
â”‚       â””â”€â”€ SimpleIndexer
â”‚           â”œâ”€â”€ __init__()         # Charge les 3 modÃ¨les ML
â”‚           â”œâ”€â”€ load_data()        # Charge DB + fit TF-IDF
â”‚           â”œâ”€â”€ process_batch()    # Encode et sauvegarde
â”‚           â”œâ”€â”€ search()           # Pipeline ML/DL complet
â”‚           â”œâ”€â”€ _semantic_search() # Bi-encoder
â”‚           â”œâ”€â”€ _bm25_search()     # TF-IDF/BM25
â”‚           â”œâ”€â”€ _rerank()          # Cross-encoder
â”‚           â””â”€â”€ _expand_query()    # Query expansion
â”‚
â”œâ”€â”€ ğŸ crawler_optimized.py        # Crawler Scrapy
â”‚   â”‚                              # - Crawl jusqu'Ã  50 pages/site
â”‚   â”‚                              # - Extraction titre, desc, images
â”‚   â”‚                              # - Envoi par batch Ã  l'indexer
â”‚   â””â”€â”€ Classes:
â”‚       â””â”€â”€ FastSpider
â”‚           â”œâ”€â”€ parse()            # Extrait contenu HTML
â”‚           â””â”€â”€ _submit_batch()    # Envoie Ã  l'indexer
â”‚
â”œâ”€â”€ ğŸŒ api_server.py               # â­ API FastAPI
â”‚   â”‚                              # Serveur backend REST
â”‚   â”œâ”€â”€ Endpoints:
â”‚   â”‚   â”œâ”€â”€ GET  /                 # Sert frontend.html
â”‚   â”‚   â”œâ”€â”€ POST /search           # Recherche ML/DL
â”‚   â”‚   â”œâ”€â”€ GET  /search/fast      # Recherche rapide
â”‚   â”‚   â”œâ”€â”€ GET  /search/precise   # Recherche prÃ©cise
â”‚   â”‚   â”œâ”€â”€ GET  /stats            # Statistiques
â”‚   â”‚   â”œâ”€â”€ POST /rebuild-index    # Recharger index
â”‚   â”‚   â”œâ”€â”€ POST /clear-cache      # Vider cache
â”‚   â”‚   â”œâ”€â”€ GET  /health           # Health check
â”‚   â”‚   â””â”€â”€ GET  /benchmark        # Benchmark des 3 modes
â”‚   â””â”€â”€ Models:
â”‚       â”œâ”€â”€ SearchRequest
â”‚       â””â”€â”€ SearchResponse
â”‚
â”œâ”€â”€ ğŸ¨ frontend.html               # â­ Interface Web
â”‚   â”‚                              # Interface complÃ¨te avec:
â”‚   â”‚                              # - SÃ©lecteur de 3 modes
â”‚   â”‚                              # - Affichage Score + URL
â”‚   â”‚                              # - Stats en temps rÃ©el
â”‚   â”‚                              # - Design moderne
â”‚   â””â”€â”€ Sections:
â”‚       â”œâ”€â”€ Mode selector          # Fast/Balanced/Precise
â”‚       â”œâ”€â”€ Search input           # Zone de recherche
â”‚       â”œâ”€â”€ Results display        # Tableau Score % + URL
â”‚       â”œâ”€â”€ Examples               # Exemples cliquables
â”‚       â””â”€â”€ ML info                # Technologies actives
â”‚
â”œâ”€â”€ ğŸ main_optimized.py           # â­ CLI Principal
â”‚   â”‚                              # Point d'entrÃ©e ligne de commande
â”‚   â””â”€â”€ Commandes:
â”‚       â”œâ”€â”€ crawl                  # Crawler et indexer
â”‚       â”œâ”€â”€ server                 # Lancer serveur API
â”‚       â”œâ”€â”€ search [query]         # Test recherche
â”‚       â”œâ”€â”€ benchmark              # Comparer les 3 modes
â”‚       â””â”€â”€ stats                  # Afficher statistiques
â”‚
â”œâ”€â”€ ğŸ app.py                      # â­ Lanceur Simple
â”‚   â”‚                              # Lance API + ouvre navigateur
â”‚   â””â”€â”€ Fonction:
â”‚       â”œâ”€â”€ start_api_server()     # Thread API
â”‚       â”œâ”€â”€ webbrowser.open()      # Ouvre navigateur
â”‚       â””â”€â”€ Maintient serveur actif
â”‚
â”œâ”€â”€ ğŸ“„ requirements.txt            # DÃ©pendances Python
â”‚   â”œâ”€â”€ scrapy>=2.11.0
â”‚   â”œâ”€â”€ sentence-transformers>=2.2.2
â”‚   â”œâ”€â”€ scikit-learn>=1.3.0
â”‚   â”œâ”€â”€ torch>=2.0.0
â”‚   â”œâ”€â”€ transformers>=4.30.0
â”‚   â”œâ”€â”€ fastapi>=0.104.0
â”‚   â””â”€â”€ uvicorn[standard]>=0.24.0
â”‚
â”œâ”€â”€ ğŸ“„ urls.txt                    # URLs Ã  crawler
â”‚   â””â”€â”€ Liste d'URLs (1 par ligne)
â”‚
â”œâ”€â”€ ğŸ“„ .gitattributes              # Config Git LFS
â”‚   â””â”€â”€ *.db filter=lfs
â”‚
â””â”€â”€ ğŸ“„ render.yml                  # Config dÃ©ploiement (optionnel)
```

---

## ğŸ”„ Flow de DonnÃ©es

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         PHASE 1: CRAWLING                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

urls.txt
   â†“
crawler_optimized.py (Scrapy)
   â†“ (par batch de 100)
indexer_optimized.py
   â”œâ”€â”€ Encode avec Bi-encoder
   â”œâ”€â”€ Calcule Page Rank
   â””â”€â”€ Sauvegarde dans emb.db


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      PHASE 2: CHARGEMENT                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

emb.db
   â†“
indexer_optimized.py
   â”œâ”€â”€ Charge embeddings
   â”œâ”€â”€ Fit TF-IDF
   â””â”€â”€ PrÃªt pour recherche


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       PHASE 3: RECHERCHE                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

User â†’ frontend.html
          â†“ (HTTP POST /search)
       api_server.py
          â†“
    indexer_optimized.py
          â”œâ”€â”€ Query Expansion (TF-IDF)
          â”œâ”€â”€ Semantic Search (Bi-encoder) â†’ Top 50
          â”œâ”€â”€ BM25 Search (TF-IDF) â†’ Top 50
          â”œâ”€â”€ RRF Fusion
          â””â”€â”€ Cross-Encoder Reranking (mode Precise)
          â†“
       api_server.py
          â†“ (JSON response)
       frontend.html
          â†“
       Affichage: Score % + URL
```

---

## ğŸš€ Commandes d'Utilisation

### 1ï¸âƒ£ Installation

```bash
# Cloner le repo
git clone <votre-repo>
cd search_engine

# CrÃ©er environnement virtuel
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows

# Installer dÃ©pendances
pip install -r requirements.txt
```

### 2ï¸âƒ£ Crawler (premiÃ¨re fois)

```bash
# Crawler avec urls.txt
python3 main_optimized.py crawl

# Ou avec fichier custom
python3 main_optimized.py crawl --urls mes_urls.txt
```

### 3ï¸âƒ£ Lancer l'application

**Option A: Lanceur simple (recommandÃ©)**
```bash
python3 app.py
# â†’ Ouvre automatiquement http://127.0.0.1:8000
```

**Option B: Serveur manuel**
```bash
python3 main_optimized.py server
# â†’ Ouvrez manuellement http://127.0.0.1:8000
```

**Option C: Port personnalisÃ©**
```bash
python3 main_optimized.py server --port 8080
```

### 4ï¸âƒ£ Tests et debug

```bash
# Statistiques
python3 main_optimized.py stats

# Test recherche
python3 main_optimized.py search "machine learning"
python3 main_optimized.py search "AI" --mode precise

# Benchmark des 3 modes
python3 main_optimized.py benchmark
```

---

## ğŸ“Š Architecture des 3 Modes

### âš¡ Fast (~50ms)

```
Query â†’ Bi-encoder â†’ SimilaritÃ© cosinus â†’ Top K
```

**Fichiers utilisÃ©s:**
- `indexer_optimized.py` : `_semantic_search()`

### âš–ï¸ Balanced (~80ms) - RecommandÃ©

```
Query
  â†“
Query Expansion
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Bi-encoder  â”‚     BM25     â”‚
â”‚   Top 50     â”‚   Top 50     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
RRF Fusion
  â†“
Top K
```

**Fichiers utilisÃ©s:**
- `indexer_optimized.py` : 
  - `_expand_query()`
  - `_semantic_search()`
  - `_bm25_search()`
  - Fusion RRF

### ğŸ¯ Precise (~150ms)

```
Balanced (ci-dessus)
  â†“
Cross-Encoder Reranking
  â†“
Top K
```

**Fichiers utilisÃ©s:**
- `indexer_optimized.py` : 
  - Tous du mode Balanced
  - `_rerank()` avec Cross-encoder

---

## ğŸ—„ï¸ Structure de la Base de DonnÃ©es

```sql
CREATE TABLE pages (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    url TEXT UNIQUE,
    title TEXT,
    description TEXT,
    image_links TEXT,              -- JSON array
    title_emb BLOB,                -- Embedding 384D (float32)
    desc_emb BLOB,                 -- Embedding 384D (float32)
    combined_text TEXT,            -- title + description
    page_rank REAL DEFAULT 0.0     -- Score de qualitÃ©
);
```

**Taille typique:**
- 40 000 pages â‰ˆ 160 MB
- 1 embedding = 384 Ã— 4 bytes = 1536 bytes
- 2 embeddings/page = 3 KB/page

---

## ğŸ§  ModÃ¨les ML/DL

### 1. Bi-Encoder
- **Nom**: `all-MiniLM-L6-v2`
- **Taille**: 23 MB
- **Dimensions**: 384
- **Vitesse**: ~2000 phrases/sec
- **Usage**: Semantic search rapide

### 2. Cross-Encoder
- **Nom**: `cross-encoder/ms-marco-MiniLM-L-6-v2`
- **Taille**: 80 MB
- **Vitesse**: ~50 paires/sec
- **Usage**: Re-ranking prÃ©cis

### 3. TF-IDF
- **Library**: scikit-learn
- **Features**: 5000 max
- **N-grams**: (1, 2)
- **Usage**: BM25 + Query Expansion

---

## ğŸ“ˆ Performance

| Mode | Temps | PrÃ©cision | Fichiers actifs |
|------|-------|-----------|-----------------|
| Fast | ~50ms | â­â­â­ | indexer (bi-encoder) |
| Balanced | ~80ms | â­â­â­â­ | indexer (bi-encoder + BM25) |
| Precise | ~150ms | â­â­â­â­â­ | indexer (tout) |

---

## ğŸ”— URLs Importantes

| URL | Description |
|-----|-------------|
| `http://127.0.0.1:8000` | Interface web (frontend.html) |
| `http://127.0.0.1:8000/docs` | Documentation API (Swagger) |
| `http://127.0.0.1:8000/stats` | Statistiques JSON |
| `http://127.0.0.1:8000/benchmark` | Benchmark des modes |
| `http://127.0.0.1:8000/health` | Health check |

---

## ğŸ“¦ Fichiers GÃ©nÃ©rÃ©s

| Fichier | Quand | Taille | Description |
|---------|-------|--------|-------------|
| `emb.db` | AprÃ¨s crawl | ~160 MB | Base principale |
| `query_cache.pkl` | AprÃ¨s recherches | ~1-10 MB | Cache LRU |
| `__pycache__/` | Au runtime | Variable | Cache Python |

---

## ğŸ¯ Points d'EntrÃ©e

### Pour l'utilisateur final:
```bash
python3 app.py
```

### Pour le dÃ©veloppeur:
```bash
# CLI complet
python3 main_optimized.py [command]

# API seule
python3 api_server.py

# Indexer seul (tests)
python3 -c "from indexer_optimized import get_indexer; i=get_indexer(); print(len(i.pages_data))"
```

---

## âœ… Checklist de VÃ©rification

- [ ] `emb.db` existe et contient des donnÃ©es (>100 MB)
- [ ] `frontend.html` est prÃ©sent
- [ ] Tous les `.py` sont dans le mÃªme dossier
- [ ] `requirements.txt` installÃ©
- [ ] Port 8000 est libre
- [ ] Python 3.8+ installÃ©

---

## ğŸ› Troubleshooting

| Erreur | Solution |
|--------|----------|
| "emb.db not found" | Lancez `python3 main_optimized.py crawl` |
| "Port 8000 in use" | Changez le port: `--port 8080` |
| "Module not found" | `pip install -r requirements.txt` |
| "Ancienne DB" | Compatible auto ou recrawlez |
| "Score column too wide" | Utilisez le nouveau `app.py` |

---

VoilÃ  la structure complÃ¨te ! ğŸ‰